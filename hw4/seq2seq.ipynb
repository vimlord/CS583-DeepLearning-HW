{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IYVWXihtBhA1"
   },
   "source": [
    "# Homework 4: Build a Seq2seq model for machine translation.\n",
    "\n",
    "### Name: Christopher Hittner\n",
    "\n",
    "### Task: Translate English to Spanish and French\n",
    "\n",
    "I pledge my honor that I have abided by the Stevens Honor System."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dwVOqjAABhA2"
   },
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read and run my code.\n",
    "2. Complete the code in Section 1.1 and Section 4.2.\n",
    "\n",
    "    * Translation English to **German** is not acceptable!!! Try another language.\n",
    "    \n",
    "3. **Make improvements.** Directly modify the code in Section 3. Do at least one of the followings. By doing more, you will get up to 2 bonus scores to the total.\n",
    "\n",
    "    * Bi-LSTM instead of LSTM\n",
    "    \n",
    "    * Multi-task learning (e.g., both English to French and English to Spanish)\n",
    "    \n",
    "    * Attention\n",
    "    \n",
    "4. Evaluate the translation using the BLEU score. \n",
    "\n",
    "    * Optional. Up to 2 bonus scores to the total.\n",
    "    \n",
    "5. Convert the notebook to .HTML file. \n",
    "\n",
    "    * The HTML file must contain the code and the output after execution.\n",
    "\n",
    "6. Put the .HTML file in your own Github repo. \n",
    "\n",
    "7. Submit the link to the HTML file to Canvas\n",
    "\n",
    "    * E.g., https://github.com/wangshusen/CS583A-2019Spring/blob/master/homework/HM4/seq2seq.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W2mEZ_OvCaH9"
   },
   "source": [
    "#### Move to Drive Directory\n",
    "\n",
    "Use this only when working on Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "colab_type": "code",
    "id": "CfKpHlzBCkQi",
    "outputId": "1b92ebca-34ef-4995-a5d9-13cc895c7ec0"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "# Mount my Google Drrive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Go to the directory with the 583 data\n",
    "os.chdir('/content/drive/My Drive/College/CS583')\n",
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4138FoJtBhA8"
   },
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "1. Download data (e.g., \"deu-eng.zip\") from http://www.manythings.org/anki/\n",
    "2. Unzip the .ZIP file.\n",
    "3. Put the .TXT file (e.g., \"deu.txt\") in the directory \"./Data/\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wz6OEGIjBhA9"
   },
   "source": [
    "### 1.1. Load and clean text\n",
    "\n",
    "Data is loaded and extracted for several language pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IhCjBu6-BhA9"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from unicodedata import normalize\n",
    "import numpy\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "\n",
    "# split a loaded document into sentences\n",
    "def to_pairs(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return pairs\n",
    "\n",
    "def clean_data(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in lines:\n",
    "        clean_pair = list()\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            #line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            #line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            #line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return numpy.array(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IC7dxJd0BhBE"
   },
   "source": [
    "The language names and training splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "y0tWhKdwBhBI",
    "outputId": "9549ee84-2718-4f83-a522-d1e49250fec2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages: fra spa\n"
     ]
    }
   ],
   "source": [
    "# e.g., filename = 'Data/deu.txt'\n",
    "langs = ['fra', 'spa']\n",
    "\n",
    "filenames = {\n",
    "    l : f'Data/{l}.txt'\n",
    "    for l in langs\n",
    "}\n",
    "\n",
    "# e.g., n_train = 20000\n",
    "n_train = {\n",
    "    ('eng', 'fra') : 80000,\n",
    "    ('eng', 'spa') : 60000\n",
    "}\n",
    "\n",
    "print('Languages:', *langs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HnaYA10vBhBM"
   },
   "outputs": [],
   "source": [
    "def load_pairs(fname):\n",
    "    # load dataset\n",
    "    doc = load_doc(fname)\n",
    "\n",
    "    # split into Language1-Language2 pairs\n",
    "    pairs = to_pairs(doc)\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def clean_data_pairs(pairs):\n",
    "    return clean_data(pairs)\n",
    "\n",
    "# Load the dataset as pairs\n",
    "pairs = {\n",
    "    ('eng', l): load_pairs(filenames[l])\n",
    "    for l in langs\n",
    "}\n",
    "    \n",
    "\n",
    "data = {\n",
    "    p: clean_data_pairs(pairs[p])\n",
    "    for p in pairs\n",
    "}\n",
    "\n",
    "# clean sentences (training data)\n",
    "clean_pairs = {\n",
    "    p: data[p][:n_train[p]]\n",
    "    for p in pairs\n",
    "}\n",
    "\n",
    "# dirty sentences (test data)\n",
    "dirty_pairs = {\n",
    "    p: data[p][n_train[p]:]\n",
    "    for p in pairs\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pK-M-PrUfFmZ"
   },
   "source": [
    "Shuffle the data, keeping identical inputs together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_ac63gYPaF4-"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for p in clean_pairs:\n",
    "    ps = clean_pairs[p]\n",
    "    \n",
    "    xs = list(set(x[0] for x in ps))\n",
    "    ys = {}\n",
    "    for x, y in ps:\n",
    "        if x not in ys:\n",
    "            ys[x] = [y]\n",
    "        else:\n",
    "            ys[x] += [y]\n",
    "    \n",
    "    # Shuffle the data.\n",
    "    random.shuffle(xs)\n",
    "    for x in xs:\n",
    "        random.shuffle(ys[x])\n",
    "    \n",
    "    # Build the data again, but shuffled. Keep identical inputs together.\n",
    "    clean_pairs[p] = numpy.array([(x,y) for x in xs for y in ys[x]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 425
    },
    "colab_type": "code",
    "id": "mHFupDyVBhBP",
    "outputId": "8376cc2f-2e9e-4bef-e6a6-9ccfb9ce7afb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng => fra\n",
      "==========\n",
      "[put your hat back on] => [remets ton chapeau]\n",
      "[keep your coat on] => [garde ton manteau sur toi]\n",
      "[he told me where to go] => [il ma dit où aller]\n",
      "[may i use some paper] => [puisje utiliser un peu de papier]\n",
      "[i am afraid to go] => [jai peur dy aller]\n",
      "[i am afraid to go] => [jai peur de my rendre]\n",
      "[im not nervous at all] => [je ne suis pas du tout nerveux]\n",
      "[im not nervous at all] => [je ne suis pas du tout nerveuse]\n",
      "[maybe next time] => [peutêtre la prochaine fois]\n",
      "[it was frightening] => [cétait effrayant]\n",
      "eng => spa\n",
      "==========\n",
      "[tom didnt come did he] => [tomás no vino]\n",
      "[i like the way tom sings] => [me agrada la forma en que canta tom]\n",
      "[maybe its a trap] => [tal vez es una trampa]\n",
      "[its a lot of work] => [es mucho trabajo]\n",
      "[she is not tall] => [ella no es alta]\n",
      "[tom and mary went outside] => [tom y mary salieron fuera]\n",
      "[tom joined us] => [tom se unió a nosotras]\n",
      "[tom joined us] => [se nos unió tom]\n",
      "[tom joined us] => [tom se unió a nosotros]\n",
      "[i plan to go there] => [pienso ir allí]\n"
     ]
    }
   ],
   "source": [
    "for p in clean_pairs:\n",
    "    print(p[0], '=>', p[1])\n",
    "    print('=' * (len(p[0]) + len(p[1]) + 4))\n",
    "    \n",
    "    for i in range(3000, 3010):\n",
    "        print('[' + clean_pairs[p][i, 0] + '] => [' + clean_pairs[p][i, 1] + ']')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0mlqYPMa4bNw"
   },
   "source": [
    "Now, we can enfore properties of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "mo_NpcTeBhBT",
    "outputId": "e96c1d1e-696b-4d43-9d21-77334a1c87fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input_texts[eng, fra]:  (80000,)\n",
      "Length of target_texts[eng, fra]: (80000,)\n",
      "Length of input_texts[eng, spa]:  (60000,)\n",
      "Length of target_texts[eng, spa]: (60000,)\n"
     ]
    }
   ],
   "source": [
    "def generate_texts(pairs):\n",
    "    input_texts = pairs[:,0]\n",
    "    target_texts = ['\\t' + text + '\\n' for text in pairs[:, 1]]\n",
    "    return input_texts, target_texts\n",
    "\n",
    "input_texts = {}\n",
    "target_texts = {}\n",
    "\n",
    "for p in pairs:\n",
    "    input_texts[p], target_texts[p] = generate_texts(clean_pairs[p])\n",
    "    \n",
    "    print(f'Length of input_texts[{p[0]}, {p[1]}]:  ' + str(input_texts[p].shape))\n",
    "    print(f'Length of target_texts[{p[0]}, {p[1]}]: ' + str(input_texts[p].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "_MXHYfeXBhBY",
    "outputId": "44e04a9b-7e37-40c3-97db-6bfe747a1df7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder data available for: eng\n",
      "Decoder data available for: fra spa\n",
      "max length of input  sentences in eng: 28\n",
      "max length of output sentences in fra: 72\n",
      "max length of output sentences in spa: 68\n"
     ]
    }
   ],
   "source": [
    "def maxlen(lines):\n",
    "    return max(len(line) for line in lines)\n",
    "\n",
    "# Build the maximum input and output lengths\n",
    "max_encoder_seq_length = {l: 0 for l in set(a for a,b in pairs)}\n",
    "max_decoder_seq_length = {l: 0 for l in set(b for a,b in pairs)}\n",
    "\n",
    "for a,b in pairs:\n",
    "    max_encoder_seq_length[a] = max(max_encoder_seq_length[a], maxlen(input_texts[a,b]))\n",
    "    max_decoder_seq_length[b] = max(max_decoder_seq_length[b], maxlen(target_texts[a,b]))\n",
    "    \n",
    "print('Encoder data available for:', *max_encoder_seq_length)\n",
    "print('Decoder data available for:', *max_decoder_seq_length)\n",
    "\n",
    "for l in max_encoder_seq_length:\n",
    "    print(f'max length of input  sentences in {l}: %d' % (max_encoder_seq_length[l]))   \n",
    "for l in max_decoder_seq_length:\n",
    "    print(f'max length of output sentences in {l}: %d' % (max_decoder_seq_length[l]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j1NPTs-6BhBc"
   },
   "source": [
    "**Remark:** To this end, you have two lists of sentences: input_texts and target_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I1Q3ISnzBhBd"
   },
   "source": [
    "## 2. Text processing\n",
    "\n",
    "### 2.1. Convert texts to sequences\n",
    "\n",
    "- Input: A list of $n$ sentences (with max length $t$).\n",
    "- It is represented by a $n\\times t$ matrix after the tokenization and zero-padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "DIAWijl4BhBe",
    "outputId": "6e2c7eb3-03bd-4d81-9bac-d6fafda2a3ee"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder tokenizer available for: eng\n",
      "Decoder tokenizer available for: fra spa\n",
      "input_token_index[eng]: {' ': 1, 'e': 2, 't': 3, 'o': 4, 'i': 5, 'a': 6, 's': 7, 'h': 8, 'n': 9, 'r': 10, 'l': 11, 'd': 12, 'm': 13, 'y': 14, 'u': 15, 'w': 16, 'g': 17, 'c': 18, 'p': 19, 'k': 20, 'f': 21, 'b': 22, 'v': 23, 'j': 24, 'x': 25, 'z': 26, 'q': 27, 'é': 28}\n",
      "target_token_index[fra]: {' ': 1, 'e': 2, 's': 3, 'a': 4, 't': 5, 'i': 6, 'u': 7, 'n': 8, 'o': 9, 'r': 10, '\\t': 11, '\\n': 12, 'l': 13, 'm': 14, 'p': 15, 'c': 16, 'd': 17, 'v': 18, 'é': 19, 'j': 20, 'q': 21, 'f': 22, 'b': 23, 'h': 24, 'g': 25, 'z': 26, 'x': 27, 'à': 28, 'ê': 29, 'è': 30, 'y': 31, 'ç': 32, 'ù': 33, 'ô': 34, 'î': 35, 'û': 36, 'â': 37, 'œ': 38, 'k': 39, 'ï': 40, 'w': 41, 'ë': 42}\n",
      "target_token_index[spa]: {' ': 1, 'e': 2, 'a': 3, 'o': 4, 's': 5, 'n': 6, 'r': 7, 't': 8, '\\t': 9, '\\n': 10, 'l': 11, 'i': 12, 'u': 13, 'm': 14, 'd': 15, 'c': 16, 'p': 17, 'b': 18, 'v': 19, 'h': 20, 'g': 21, 'q': 22, 'é': 23, 'y': 24, 'á': 25, 'í': 26, 'ó': 27, 'f': 28, 'j': 29, 'z': 30, 'ñ': 31, 'ú': 32, 'x': 33, 'k': 34, 'w': 35, 'ü': 36}\n",
      "shape of encoder_input_seq[eng, fra]: (80000, 28)\n",
      "shape of input_token_index[eng, fra]: 28\n",
      "shape of decoder_input_seq[eng, fra]: (80000, 72)\n",
      "shape of target_token_index[eng, fra]: 42\n",
      "shape of encoder_input_seq[eng, spa]: (60000, 28)\n",
      "shape of input_token_index[eng, spa]: 28\n",
      "shape of decoder_input_seq[eng, spa]: (60000, 68)\n",
      "shape of target_token_index[eng, spa]: 36\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# encode and pad sequences\n",
    "def text2sequences(max_len, lines):\n",
    "    tokenizer = Tokenizer(char_level=True, filters='')\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    seqs_pad = encode_text(lines, tokenizer, max_len)\n",
    "    return seqs_pad, tokenizer.word_index, tokenizer\n",
    "\n",
    "def encode_text(texts, tokenizer, max_len):\n",
    "    seqs = tokenizer.texts_to_sequences(texts)\n",
    "    seqs_pad = pad_sequences(seqs, maxlen=max_len, padding='post')\n",
    "    return seqs_pad\n",
    "\n",
    "encoder_input_seq = {}\n",
    "input_token_index = {}\n",
    "\n",
    "decoder_input_seq = {}\n",
    "target_token_index = {}\n",
    "\n",
    "\n",
    "# Generate all of the tokenizers\n",
    "encoder_tokenizer = {}\n",
    "decoder_tokenizer = {}\n",
    "\n",
    "for a,b in clean_pairs:\n",
    "    inputs = input_texts[a,b]\n",
    "    targets = target_texts[a,b]\n",
    "    \n",
    "    # Augment the encoding tokenizer\n",
    "    if a not in encoder_tokenizer:\n",
    "        encoder_tokenizer[a] = tok = Tokenizer(char_level=True, filters='')\n",
    "    else:\n",
    "        tok = encoder_tokenizer[a]\n",
    "        tok.fit_on_texts(inputs)\n",
    "    \n",
    "    # Augment the decoding tokenizer\n",
    "    if b not in decoder_tokenizer:\n",
    "        decoder_tokenizer[b] = tok = Tokenizer(char_level=True, filters='')\n",
    "    else:\n",
    "        tok = decoder_tokenizer[b]\n",
    "    tok.fit_on_texts(targets)\n",
    "\n",
    "\n",
    "print('Encoder tokenizer available for:', *encoder_tokenizer)\n",
    "print('Decoder tokenizer available for:', *decoder_tokenizer)\n",
    "\n",
    "# Generate the token_indices (map tokens to indices)\n",
    "input_token_index = {l: encoder_tokenizer[l].word_index for l in encoder_tokenizer}\n",
    "target_token_index = {l: decoder_tokenizer[l].word_index for l in decoder_tokenizer}\n",
    "\n",
    "for l in input_token_index:\n",
    "    print(f'input_token_index[{l}]:', input_token_index[l])\n",
    "    \n",
    "for l in target_token_index:\n",
    "    print(f'target_token_index[{l}]:', target_token_index[l])\n",
    "\n",
    "encoder_input_seq = {\n",
    "    p: encode_text(input_texts[p], encoder_tokenizer[p[0]], max_encoder_seq_length[p[0]])\n",
    "    for p in clean_pairs\n",
    "}\n",
    "\n",
    "decoder_input_seq = {\n",
    "    p: encode_text(target_texts[p], decoder_tokenizer[p[1]], max_decoder_seq_length[p[1]])\n",
    "    for p in clean_pairs\n",
    "}\n",
    "\n",
    "decoder_target_seq = {}\n",
    "for l in clean_pairs:\n",
    "    decoder_target_seq[l] = numpy.zeros(decoder_input_seq[l].shape)\n",
    "    decoder_target_seq[l][:, 0:-1] = decoder_input_seq[l][:, 1:]\n",
    "\n",
    "# Print the data shapes\n",
    "for p in clean_pairs:\n",
    "    print(f'shape of encoder_input_seq[{p[0]}, {p[1]}]: ' + str(encoder_input_seq[p].shape))\n",
    "    print(f'shape of input_token_index[{p[0]}, {p[1]}]: ' + str(len(input_token_index[p[0]])))\n",
    "    print(f'shape of decoder_input_seq[{p[0]}, {p[1]}]: ' + str(decoder_input_seq[p].shape))\n",
    "    print(f'shape of target_token_index[{p[0]}, {p[1]}]: ' + str(len(target_token_index[p[1]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "UJ9gdCTPBhBl",
    "outputId": "fafd7bf7-b3f0-4388-c479-0a004a7d0a7d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_encoder_tokens[eng]: 29\n",
      "num_decoder_tokens[fra]: 43\n",
      "num_decoder_tokens[spa]: 37\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens = {\n",
    "    l: len(input_token_index[l]) + 1\n",
    "    for l in encoder_tokenizer\n",
    "}\n",
    "\n",
    "num_decoder_tokens = {\n",
    "    l: len(target_token_index[l]) + 1\n",
    "    for l in decoder_tokenizer\n",
    "}\n",
    "\n",
    "for l in encoder_tokenizer:\n",
    "    print(f'num_encoder_tokens[{l}]: ' + str(num_encoder_tokens[l]))\n",
    "    \n",
    "for l in decoder_tokenizer:\n",
    "    print(f'num_decoder_tokens[{l}]: ' + str(num_decoder_tokens[l]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H5nMTcGQBhBo"
   },
   "source": [
    "**Remark:** To this end, the input language and target language texts are converted to 2 matrices. \n",
    "\n",
    "- Their number of rows are both n_train.\n",
    "- Their number of columns are respective max_encoder_seq_length and max_decoder_seq_length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JpszG0OHBhBp"
   },
   "source": [
    "The followings print a sentence and its representation as a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "Zgc8Sf59BhBq",
    "outputId": "9920920e-b818-4148-9c01-2ab970776c03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng => fra : '\\tcomme cest embarrassant\\n'\n",
      "eng => spa : '\\tyo no sé su verdadero nombre\\n'\n"
     ]
    }
   ],
   "source": [
    "for l in clean_pairs:\n",
    "    print(l[0], '=>', l[1], ':', repr(target_texts[l][100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "7astGAqkBhBu",
    "outputId": "6e4f9d2c-2f52-4ac1-9f5f-22cac9d36a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng => fra : array([11, 16,  9, 14, 14,  2,  1, 16,  2,  3,  5,  1,  2, 14, 23,  4, 10,\n",
      "       10,  4,  3,  3,  4,  8,  5, 12,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0], dtype=int32)\n",
      "eng => spa : array([ 9, 24,  4,  1,  6,  4,  1,  5, 23,  1,  5, 13,  1, 19,  2,  7, 15,\n",
      "        3, 15,  2,  7,  4,  1,  6,  4, 14, 18,  7,  2, 10,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "      dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for l in clean_pairs:\n",
    "    print(l[0], '=>', l[1], ':', repr(decoder_input_seq[l][100, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xUTd8EY-BhBz"
   },
   "source": [
    "## 2.2. One-hot encode\n",
    "\n",
    "- Input: A list of $n$ sentences (with max length $t$).\n",
    "- It is represented by a $n\\times t$ matrix after the tokenization and zero-padding.\n",
    "- It is represented by a $n\\times t \\times v$ tensor ($t$ is the number of unique chars) after the one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_i5OecFFBhB0"
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# one hot encode target sequence\n",
    "def onehot_encode(sequences, max_len, vocab_size):\n",
    "    n = len(sequences)\n",
    "    data = numpy.zeros((n, max_len, vocab_size))\n",
    "    for i in range(n):\n",
    "        data[i, :, :] = to_categorical(sequences[i], num_classes=vocab_size)\n",
    "    return data\n",
    "\n",
    "\"\"\"\n",
    "encoder_input_data = {\n",
    "    p: onehot_encode(encoder_input_seq[p], max_encoder_seq_length[p[0]], num_encoder_tokens[p[0]])\n",
    "    for p in clean_pairs\n",
    "}\n",
    "decoder_input_data = {\n",
    "    p: onehot_encode(decoder_input_seq[p], max_decoder_seq_length[p[1]], num_decoder_tokens[p[1]])\n",
    "    for p in clean_pairs\n",
    "}\n",
    "\n",
    "for l in clean_pairs:\n",
    "    print(f'size of encoder_input_data[{l[0]}, {l[1]}]:', encoder_input_data[l].shape)\n",
    "    print(f'size of decoder_input_data[{l[0]}, {l[1]}]:', decoder_input_data[l].shape) \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "decoder_target_data = {}\n",
    "    \n",
    "for l in clean_pairs:\n",
    "    decoder_target_data[l] = onehot_encode(decoder_target_seq[l], \n",
    "                                        max_decoder_seq_length[l[1]], \n",
    "                                        num_decoder_tokens[l[1]])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "denACvyquHvl"
   },
   "source": [
    "## 2.3. Data Batching\n",
    "\n",
    "When training the modes, we want to use all of the dataset to train at each iteration. Hence, we can say that for each batch of a translation direction and x-y pairing, fit the translation to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sw3O_afoumcH"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class MultiDataGenerator:\n",
    "    def __init__(self, Xs, Ys, batch_size=64):\n",
    "        self.Xs = Xs\n",
    "        self.Ys = Ys\n",
    "        self.indices = None\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.X = None\n",
    "        self.Y = None\n",
    "\n",
    "    def __iter__(self):\n",
    "        Xs = self.Xs\n",
    "        Ys = self.Ys\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        slices = []\n",
    "        \n",
    "        X = []\n",
    "        Y = []\n",
    "\n",
    "        # Select random slices\n",
    "        for i in Xs:\n",
    "            ys = Ys[i]\n",
    "            \n",
    "            # Shuffle each dataset\n",
    "            idxs = list(range(len(ys)))\n",
    "            random.shuffle(idxs)\n",
    "\n",
    "            # Select slices\n",
    "            slices.extend([(i, idxs[j-batch_size:j])\n",
    "                for j in range(batch_size, len(ys)+1, batch_size)])\n",
    "            \n",
    "\n",
    "        random.shuffle(slices)\n",
    "            \n",
    "        for i, s in slices:\n",
    "            p = i\n",
    "            X = [x[s] for x in Xs[i]]\n",
    "            Y = Ys[i][s]\n",
    "            \n",
    "            yield p, X, Y\n",
    "\n",
    "    def __len__(self):\n",
    "        # The size is the number of possible batches\n",
    "        return sum(len(self.Ys[p]) // self.batch_size for p in self.Ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "6X423itruq4X",
    "outputId": "8190b37e-aa86-4bcf-c2c8-7e935b8fef64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 64\n",
      "Training batch count: 1750\n",
      "Validation batch count: 437\n"
     ]
    }
   ],
   "source": [
    "Xs = {\n",
    "    #p: [encoder_input_data[p], decoder_input_data[p]]\n",
    "    p: [encoder_input_seq[p], decoder_input_seq[p]]\n",
    "    for p in clean_pairs\n",
    "}\n",
    "\n",
    "Ys = {\n",
    "    p: decoder_target_data[p]\n",
    "    for p in clean_pairs\n",
    "}\n",
    "\n",
    "# Split the data\n",
    "validation_split = 0.2\n",
    "batch_size = 64\n",
    "\n",
    "Xs_train = {\n",
    "    p: [X[:int((1-validation_split)*len(X))] for X in Xs[p]]\n",
    "    for p in Xs\n",
    "}\n",
    "Xs_valid = {\n",
    "    p: [X[int((1-validation_split)*len(X)):] for X in Xs[p]]\n",
    "    for p in Xs\n",
    "}\n",
    "\n",
    "Ys_train = {\n",
    "    p: Ys[p][:int((1-validation_split)*len(Ys[p]))]\n",
    "    for p in Ys\n",
    "}\n",
    "Ys_valid = {\n",
    "    p: Ys[p][int((1-validation_split)*len(Ys[p])):]\n",
    "    for p in Ys\n",
    "}\n",
    "\n",
    "gen_train = MultiDataGenerator(Xs_train, Ys_train, batch_size)\n",
    "gen_valid = MultiDataGenerator(Xs_valid, Ys_valid, batch_size)\n",
    "\n",
    "gen_train.__iter__()\n",
    "\n",
    "print('Batch size:', batch_size)\n",
    "print('Training batch count:', len(gen_train))\n",
    "print('Validation batch count:', len(gen_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gTUqzlC_BhB4"
   },
   "source": [
    "## 3. Build the networks (for training)\n",
    "\n",
    "- Build encoder, decoder, and connect the two modules to get \"model\". \n",
    "\n",
    "- Fit the model on the bilingual data to train the parameters in the encoder and decoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VirApHq_cDYF"
   },
   "source": [
    "### 3.0. Miscellaneous layers\n",
    "\n",
    "To build our models, special layers will be needed. These include:\n",
    "\n",
    "* Attention context generation\n",
    "* Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XPrKxi7_HM3D"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Activation, Dense, Permute, Lambda, Layer, Softmax, Input\n",
    "import keras.backend as K\n",
    "\n",
    "def general_score(x, h):\n",
    "    \"\"\" Attention score computed as such:\n",
    "    score(x, h) = x^T Wh\n",
    "    \"\"\"\n",
    "    dim = x.shape.as_list()[-1]\n",
    "    \n",
    "    # Weighted state vectors\n",
    "    H = Dense(dim)(h)\n",
    "    \n",
    "    # Transpose the inputs\n",
    "    hT = Permute((2,1))(H)\n",
    "\n",
    "    # Dot product (yields score alpha)\n",
    "    a = Lambda(lambda z: K.batch_dot(z[0], z[1]), name='attention')([x, hT])\n",
    "    a = Softmax(axis=2)(a)\n",
    "    \n",
    "    return a\n",
    "\n",
    "def Attention(x, h, score=general_score, concat_original=True):\n",
    "    \"\"\" Attention using the weighted dot score.\n",
    "    \"\"\"\n",
    "    a = score(x, h)\n",
    "    \n",
    "    # Compute the context for each position\n",
    "    c = Lambda(lambda x: K.batch_dot(x[0], x[1]), name='context')([a, h])\n",
    "    \n",
    "    # Concatenate to the original input\n",
    "    if concat_original:\n",
    "        return Concatenate(axis=-1)([c, x])\n",
    "    else:\n",
    "        return c\n",
    "\n",
    "def AttentionCell(num_decoder_tokens, latent_dim):\n",
    "    # inputs of the decoder network\n",
    "    x = Input(shape=(1, num_decoder_tokens + latent_dim))\n",
    "    h = Input(shape=(latent_dim,))\n",
    "    c = Input(shape=(latent_dim,))\n",
    "    hs = Input(shape=(None, latent_dim))\n",
    "\n",
    "    # set the LSTM layer\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=False,\n",
    "                        return_state=True, dropout=0.5,\n",
    "                        input_shape=x.shape[1:])\n",
    "    y, _h, _c = decoder_lstm(x, initial_state=[h, c])\n",
    "\n",
    "    # Compute attentional context\n",
    "    context = Attention(y, hs, concat_original=False)\n",
    "\n",
    "    return Model([x, h, c, hs], [y, _h, _c, context])\n",
    "\n",
    "def AttentionRNN(vocab_size, latent_dim, length):\n",
    "    \"\"\" Creates an unfolded RNN for training with Attention\n",
    "    x      - The input sequence\n",
    "    state  - The state input for the base RNN\n",
    "    hs     - The encoder states for generating the attention vector\n",
    "    length - The length of the RNN unfolding\n",
    "    \"\"\"\n",
    "    x = Input(shape=(length, vocab_size))\n",
    "    h = h0 = Input(shape=(latent_dim,))\n",
    "    c = c0 = Input(shape=(latent_dim,))\n",
    "    hs = Input(shape=(None, latent_dim))\n",
    "\n",
    "    cell = AttentionCell(vocab_size, latent_dim)\n",
    "\n",
    "    # Initial context vector\n",
    "    ctx = zero = K.zeros((1, latent_dim))\n",
    "    ctx = Lambda(lambda xs: K.tile(ctx, [K.shape(x)[0], 1]))(x)\n",
    "\n",
    "    ys = []\n",
    "\n",
    "    for i in range(length):\n",
    "        # Get the current index\n",
    "        z = Lambda(lambda z: z[:,i])(x)\n",
    "        # Apply the context vector\n",
    "        z = Concatenate(axis=-1)([z, ctx])\n",
    "        # Expand to allow for application to the LSTM\n",
    "        z = Lambda(lambda z: K.expand_dims(z, 1))(z)\n",
    "        # Apply the attentive LSTM layer to one input.\n",
    "        # Given state and encoder state, outputs new state and context\n",
    "        y, h, c, ctx = cell([z, h, c, hs])\n",
    "\n",
    "        # Save the output\n",
    "        ys.append(y)\n",
    "\n",
    "    # Concatenate all outputs\n",
    "    ys = list(map(Lambda(lambda z: K.expand_dims(z, 1)), ys))\n",
    "    y = Concatenate(axis=1)(ys)\n",
    "\n",
    "    # Return the end result\n",
    "    return Model([x, h0, c0, hs], [y, h, c, ctx]), cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b5sEi2kRcQVK"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional, Concatenate, LSTM\n",
    "\n",
    "def Bidirect(lstm, encoder_inputs):\n",
    "    \"\"\" Handles the functional call for creating a Bidirectional LSTM all in one.\n",
    "    \"\"\"\n",
    "    encoder_bilstm = Bidirectional(lstm)\n",
    "    \n",
    "    ys = encoder_bilstm(encoder_inputs)\n",
    "    \n",
    "    y = ys[0]\n",
    "    hs = ys[1:]\n",
    "    \n",
    "    ys = [y]\n",
    "    \n",
    "    # Concatenate the states\n",
    "    c = len(hs) // 2\n",
    "    for i in range(c):\n",
    "        h = Concatenate()([hs[i], hs[i+c]])\n",
    "        ys.append(h)\n",
    "\n",
    "    return ys\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ziAgZh1yBhB5"
   },
   "source": [
    "### 3.1. Encoder network\n",
    "\n",
    "- Input:  one-hot encode of the input language\n",
    "\n",
    "- Return: \n",
    "\n",
    "    -- output (all the hidden states   $h_1, \\cdots , h_t$) are always discarded\n",
    "    \n",
    "    -- the final hidden state  $h_t$\n",
    "    \n",
    "    -- the final conveyor belt $c_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "-CjeFtGgBhB5",
    "outputId": "0b044950-6e98-478d-91c3-cfb8a9579738"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, GRU, Embedding\n",
    "from keras.models import Model\n",
    "\n",
    "import keras.backend as K\n",
    "\n",
    "latent_dim = 256\n",
    "\n",
    "def build_encoder(num_encoder_tokens, name='encoder'):\n",
    "    # inputs of the encoder network\n",
    "    encoder_inputs = Input(shape=(None,),  name=f'{name}_inputs', dtype='int32')\n",
    "    \n",
    "    \n",
    "    inputs = Embedding(num_encoder_tokens, num_encoder_tokens,\n",
    "                       embeddings_initializer='identity', trainable=False)(encoder_inputs)\n",
    "\n",
    "    # set the LSTM layer\n",
    "    encoder_lstm = LSTM(latent_dim // 2, return_state=True, return_sequences=True,\n",
    "                        dropout=0.5, name='encoder_lstm')\n",
    "    outputs = Bidirect(encoder_lstm, inputs) #encoder_lstm(encoder_inputs)\n",
    "\n",
    "    # build the encoder network model\n",
    "    encoder_model = Model(inputs=encoder_inputs, \n",
    "                          outputs=outputs,\n",
    "                          name='encoder')\n",
    "\n",
    "    return encoder_model\n",
    "\n",
    "encoder_model = {\n",
    "    l: build_encoder(num_encoder_tokens[l], name=f'encoder_{l}')\n",
    "    for l in encoder_tokenizer\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hnEi1VrzBhB8"
   },
   "source": [
    "Print a summary and save the encoder network structure to \"./encoder.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S2kvZ392BhB9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "for l in encoder_tokenizer:\n",
    "    SVG(model_to_dot(encoder_model[l], show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "    plot_model(\n",
    "        model=encoder_model[l], show_shapes=False,\n",
    "        to_file=f'encoder_{l[0]}_{l[1]}.pdf'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "colab_type": "code",
    "id": "RT62RSuxBhCA",
    "outputId": "fee61975-c3b1-4498-c63f-0ee9e17feb79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eng\n",
      "===\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_eng_inputs (InputLayer) (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 29)     841         encoder_eng_inputs[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, None, 256),  121344      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 256)          0           bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][2]            \n",
      "==================================================================================================\n",
      "Total params: 122,185\n",
      "Trainable params: 121,344\n",
      "Non-trainable params: 841\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for l in encoder_tokenizer:\n",
    "    print(l)\n",
    "    print('=' * len(l))\n",
    "    encoder_model[l].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JZgQYqN_BhCE"
   },
   "source": [
    "### 3.2. Decoder network\n",
    "\n",
    "- Inputs:  \n",
    "\n",
    "    -- one-hot encode of the target language\n",
    "    \n",
    "    -- The initial hidden state $h_t$ \n",
    "    \n",
    "    -- The initial conveyor belt $c_t$ \n",
    "\n",
    "- Return: \n",
    "\n",
    "    -- output (all the hidden states) $h_1, \\cdots , h_t$\n",
    "\n",
    "    -- the final hidden state  $h_t$ (discarded in the training and used in the prediction)\n",
    "    \n",
    "    -- the final conveyor belt $c_t$ (discarded in the training and used in the prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FQRzaUyqBhCE"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "\n",
    "def build_decoder(num_decoder_tokens, name='decoder'):\n",
    "    # inputs of the decoder network\n",
    "    decoder_input_s = Input(shape=(None, latent_dim), name=f'{name}_input_s')\n",
    "    decoder_input_h = Input(shape=(latent_dim,), name=f'{name}_input_h')\n",
    "    decoder_input_c = Input(shape=(latent_dim,), name=f'{name}_input_c')\n",
    "    decoder_input_x = Input(shape=(None,), name='decoder_input_x')\n",
    "    \n",
    "    state_inputs = [\n",
    "        decoder_input_h,\n",
    "        decoder_input_c,\n",
    "    ]\n",
    "    \n",
    "    input_x = Embedding(num_decoder_tokens, num_decoder_tokens,\n",
    "                        embeddings_initializer='identity', trainable=False)(decoder_input_x)\n",
    "    \n",
    "    # set the LSTM layer\n",
    "    decoder_lstm = LSTM(latent_dim, return_sequences=True, \n",
    "                        return_state=True, dropout=0.5, name='decoder_lstm')\n",
    "    decoder_lstm_outputs = decoder_lstm(input_x, \n",
    "                                                          initial_state=state_inputs)\n",
    "    \n",
    "    decoder_lstm_outputs, state_outputs = decoder_lstm_outputs[0], decoder_lstm_outputs[1:]\n",
    "    \n",
    "    # set the attention layer\n",
    "    attn_outputs = Attention(decoder_lstm_outputs, decoder_input_s)\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    decoder_attn, attn_cell = AttentionRNN(32, latent_dim, max_decoder_seq_length[name[8:]])\n",
    "    attn_outputs, state_h, state_c, _ = decoder_attn([input_x, decoder_input_h, decoder_input_c, decoder_input_s])\n",
    "    \"\"\"\n",
    "    \n",
    "    # set the dense layer\n",
    "    decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
    "    decoder_outputs = decoder_dense(attn_outputs)\n",
    "    \n",
    "    inputs = [decoder_input_x, decoder_input_s] + state_inputs\n",
    "\n",
    "    outputs = [decoder_outputs] + state_outputs\n",
    "    \n",
    "    # build the decoder network model\n",
    "    decoder_model = Model(\n",
    "        inputs=inputs,\n",
    "        outputs=outputs,         \n",
    "        name='decoder')\n",
    "    \n",
    "    return decoder_model#, attn_cell\n",
    "\n",
    "decoder_model = {\n",
    "    l: build_decoder(num_decoder_tokens[l], name=f'decoder_{l}')\n",
    "    for l in decoder_tokenizer\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z-WavtoeBhCG"
   },
   "source": [
    "Print a summary and save the encoder network structure to \"./decoder.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MDV_o-18BhCH"
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "for l in decoder_tokenizer:\n",
    "    SVG(model_to_dot(decoder_model[l], show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "    plot_model(\n",
    "        model=decoder_model[l], show_shapes=False,\n",
    "        to_file=f'decoder_{l}.pdf'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1295
    },
    "colab_type": "code",
    "id": "aD1JiR9wBhCN",
    "outputId": "c2cf4c87-35f0-4c34-8207-2150302266ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fra\n",
      "===\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input_x (InputLayer)    (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_fra_input_s (InputLayer (None, None, 256)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 43)     1849        decoder_input_x[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_fra_input_h (InputLayer (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 256)    65792       decoder_fra_input_s[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (GRU)              [(None, None, 256),  230400      embedding_2[0][0]                \n",
      "                                                                 decoder_fra_input_h[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, 256, None)    0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention (Lambda)              (None, None, None)   0           decoder_lstm[0][0]               \n",
      "                                                                 permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_1 (Softmax)             (None, None, None)   0           attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "context (Lambda)                (None, None, 256)    0           softmax_1[0][0]                  \n",
      "                                                                 decoder_fra_input_s[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, 512)    0           context[0][0]                    \n",
      "                                                                 decoder_lstm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 43)     22059       concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 320,100\n",
      "Trainable params: 318,251\n",
      "Non-trainable params: 1,849\n",
      "__________________________________________________________________________________________________\n",
      "spa\n",
      "===\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input_x (InputLayer)    (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_spa_input_s (InputLayer (None, None, 256)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 37)     1369        decoder_input_x[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_spa_input_h (InputLayer (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 256)    65792       decoder_spa_input_s[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (GRU)              [(None, None, 256),  225792      embedding_3[0][0]                \n",
      "                                                                 decoder_spa_input_h[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "permute_2 (Permute)             (None, 256, None)    0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention (Lambda)              (None, None, None)   0           decoder_lstm[0][0]               \n",
      "                                                                 permute_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "softmax_2 (Softmax)             (None, None, None)   0           attention[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "context (Lambda)                (None, None, 256)    0           softmax_2[0][0]                  \n",
      "                                                                 decoder_spa_input_s[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, 512)    0           context[0][0]                    \n",
      "                                                                 decoder_lstm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 37)     18981       concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 311,934\n",
      "Trainable params: 310,565\n",
      "Non-trainable params: 1,369\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for l in decoder_tokenizer:\n",
    "    print(l)\n",
    "    print('=' * len(l))\n",
    "    decoder_model[l].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XXGoyHU5BhCT"
   },
   "source": [
    "### 3.3. Connect the encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hxFdBMzLBhCW"
   },
   "outputs": [],
   "source": [
    "def build_translator(src, tgt, ename='encoder', dname='decoder'):\n",
    "    encoder = encoder_model[src]\n",
    "    decoder = decoder_model[tgt]\n",
    "    \n",
    "    # input layers\n",
    "    encoder_input_x = Input(shape=(None,), name=f'{ename}_input_x')\n",
    "    decoder_input_x = Input(shape=(None,), name=f'{dname}_input_x')\n",
    "\n",
    "    # connect encoder to decoder\n",
    "    encoder_final_states = encoder([encoder_input_x])\n",
    "    \n",
    "    x = [decoder_input_x] + encoder_final_states\n",
    "    \n",
    "    decoder_pred = decoder(x)[0]\n",
    "\n",
    "    return Model(inputs=[encoder_input_x, decoder_input_x], \n",
    "                  outputs=decoder_pred, \n",
    "                  name=f'{src}2{tgt}_training')\n",
    "\n",
    "model = {\n",
    "    l: build_translator(l[0], l[1], ename=f'enc_{l[0]}_{l[1]}', dname=f'dec_{l[0]}_{l[1]}')\n",
    "    for l in clean_pairs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oBgwmX4DBhCb"
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "for l in clean_pairs:\n",
    "    SVG(model_to_dot(model[l], show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "    plot_model(\n",
    "        model=model[l], show_shapes=False,\n",
    "        to_file=f'model_training_{l[0]}_{l[1]}.pdf'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 683
    },
    "colab_type": "code",
    "id": "YzYR-UAIBhCd",
    "outputId": "4c86107a-8a2b-4378-8ac1-3b46ccbc4a20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('eng', 'fra')\n",
      "==============\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_eng_fra_input_x (InputLayer (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_eng_fra_input_x (InputLayer (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, None, 256),  122185      enc_eng_fra_input_x[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 [(None, None, 43), ( 320100      dec_eng_fra_input_x[0][0]        \n",
      "                                                                 encoder[1][0]                    \n",
      "                                                                 encoder[1][1]                    \n",
      "==================================================================================================\n",
      "Total params: 442,285\n",
      "Trainable params: 439,595\n",
      "Non-trainable params: 2,690\n",
      "__________________________________________________________________________________________________\n",
      "('eng', 'spa')\n",
      "==============\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_eng_spa_input_x (InputLayer (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dec_eng_spa_input_x (InputLayer (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, None, 256),  122185      enc_eng_spa_input_x[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 [(None, None, 37), ( 311934      dec_eng_spa_input_x[0][0]        \n",
      "                                                                 encoder[2][0]                    \n",
      "                                                                 encoder[2][1]                    \n",
      "==================================================================================================\n",
      "Total params: 434,119\n",
      "Trainable params: 431,909\n",
      "Non-trainable params: 2,210\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for l in clean_pairs:\n",
    "    print(l)\n",
    "    print('=' * len(str(l)))\n",
    "    model[l].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Rf8wIkCBhCl"
   },
   "source": [
    "### 3.5. Fit the model on the bilingual dataset\n",
    "\n",
    "- encoder_input_data: one-hot encode of the input language\n",
    "\n",
    "- decoder_input_data: one-hot encode of the input language\n",
    "\n",
    "- decoder_target_data: labels (left shift of decoder_input_data)\n",
    "\n",
    "- tune the hyper-parameters\n",
    "\n",
    "- stop when the validation loss stop decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "fCgRqTy1BhCm",
    "outputId": "fb8dd498-137f-44da-b5f2-fe43b87fd3fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of decoder_target_data[eng, fra]: (80000, 72, 43)\n",
      "shape of decoder_target_data[eng, spa]: (60000, 68, 37)\n"
     ]
    }
   ],
   "source": [
    "for l in clean_pairs:\n",
    "    #print(f'shape of encoder_input_data[{l[0]}, {l[1]}]:', str(encoder_input_data[l].shape))\n",
    "    #print(f'shape of decoder_input_data[{l[0]}, {l[1]}]:', str(decoder_input_data[l].shape))\n",
    "    print(f'shape of decoder_target_data[{l[0]}, {l[1]}]:', str(decoder_target_data[l].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "mDgHVsVd4sqd",
    "outputId": "ff48a6c7-8c80-4ef4-9bda-beafe4957c70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling eng to fra translator\n",
      "Compiling eng to spa translator\n"
     ]
    }
   ],
   "source": [
    "for l in clean_pairs:\n",
    "    print('Compiling', l[0], 'to', l[1], 'translator')\n",
    "    model[l].compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 921
    },
    "colab_type": "code",
    "id": "WQa1-E2TBhCu",
    "outputId": "7f18783a-e1c2-4bfd-a56b-79738d102a41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.879779 loss[eng, spa]: 0.879420\n",
      "val_loss[eng, fra]: 0.662418\n",
      "val_loss[eng, spa]: 0.685822\n",
      "Time elapsed: 442.06355476379395 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vimlord/.local/lib/python3.7/site-packages/keras/engine/network.py:877: UserWarning: Layer decoder_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'decoder_fra_input_h_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'decoder_fra_input_c_2:0' shape=(?, 256) dtype=float32>], 'mask': [None, None, None]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n",
      "/home/vimlord/.local/lib/python3.7/site-packages/keras/engine/network.py:877: UserWarning: Layer decoder_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'decoder_spa_input_h_1:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'decoder_spa_input_c_1:0' shape=(?, 256) dtype=float32>], 'mask': [None, None, None]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.663258 loss[eng, spa]: 0.689995\n",
      "val_loss[eng, fra]: 0.554021\n",
      "val_loss[eng, spa]: 0.596848\n",
      "Time elapsed: 402.1472773551941 seconds\n",
      "Epoch 3 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.584082 loss[eng, spa]: 0.622725\n",
      "val_loss[eng, fra]: 0.498703\n",
      "val_loss[eng, spa]: 0.541984\n",
      "Time elapsed: 343.5735728740692 seconds\n",
      "Epoch 4 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.534586 loss[eng, spa]: 0.580182\n",
      "val_loss[eng, fra]: 0.459998\n",
      "val_loss[eng, spa]: 0.506861\n",
      "Time elapsed: 388.8891043663025 seconds\n",
      "Epoch 5 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.500305 loss[eng, spa]: 0.548796\n",
      "val_loss[eng, fra]: 0.429379\n",
      "val_loss[eng, spa]: 0.481494\n",
      "Time elapsed: 501.9429557323456 seconds\n",
      "Epoch 6 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.474958 loss[eng, spa]: 0.523998\n",
      "val_loss[eng, fra]: 0.413335\n",
      "val_loss[eng, spa]: 0.456932\n",
      "Time elapsed: 460.7687494754791 seconds\n",
      "Epoch 7 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.455232 loss[eng, spa]: 0.503804\n",
      "val_loss[eng, fra]: 0.396532\n",
      "val_loss[eng, spa]: 0.441619\n",
      "Time elapsed: 359.367014169693 seconds\n",
      "Epoch 8 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.439056 loss[eng, spa]: 0.487161\n",
      "val_loss[eng, fra]: 0.381884\n",
      "val_loss[eng, spa]: 0.423342\n",
      "Time elapsed: 462.97649097442627 seconds\n",
      "Epoch 9 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.425345 loss[eng, spa]: 0.471565\n",
      "val_loss[eng, fra]: 0.371995\n",
      "val_loss[eng, spa]: 0.411824\n",
      "Time elapsed: 561.52814245224 seconds\n",
      "Epoch 10 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.413553 loss[eng, spa]: 0.458637\n",
      "val_loss[eng, fra]: 0.357106\n",
      "val_loss[eng, spa]: 0.399328\n",
      "Time elapsed: 435.25334548950195 seconds\n",
      "Epoch 11 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.403534 loss[eng, spa]: 0.447139\n",
      "val_loss[eng, fra]: 0.348566\n",
      "val_loss[eng, spa]: 0.390610\n",
      "Time elapsed: 384.08709168434143 seconds\n",
      "Epoch 12 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.393555 loss[eng, spa]: 0.436943\n",
      "val_loss[eng, fra]: 0.339414\n",
      "val_loss[eng, spa]: 0.382394\n",
      "Time elapsed: 384.33313179016113 seconds\n",
      "Epoch 13 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.384696 loss[eng, spa]: 0.427798\n",
      "val_loss[eng, fra]: 0.331601\n",
      "val_loss[eng, spa]: 0.373864\n",
      "Time elapsed: 374.3038763999939 seconds\n",
      "Epoch 14 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.375273 loss[eng, spa]: 0.419862\n",
      "val_loss[eng, fra]: 0.321412\n",
      "val_loss[eng, spa]: 0.365283\n",
      "Time elapsed: 340.2189517021179 seconds\n",
      "Epoch 15 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.365652 loss[eng, spa]: 0.412425\n",
      "val_loss[eng, fra]: 0.313328\n",
      "val_loss[eng, spa]: 0.360234\n",
      "Time elapsed: 417.0479521751404 seconds\n",
      "Epoch 16 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.357117 loss[eng, spa]: 0.406007\n",
      "val_loss[eng, fra]: 0.305209\n",
      "val_loss[eng, spa]: 0.355363\n",
      "Time elapsed: 347.5564823150635 seconds\n",
      "Epoch 17 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.349712 loss[eng, spa]: 0.399338\n",
      "val_loss[eng, fra]: 0.297058\n",
      "val_loss[eng, spa]: 0.348194\n",
      "Time elapsed: 345.30447816848755 seconds\n",
      "Epoch 18 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.342674 loss[eng, spa]: 0.393764\n",
      "val_loss[eng, fra]: 0.292788\n",
      "val_loss[eng, spa]: 0.343833\n",
      "Time elapsed: 344.894921541214 seconds\n",
      "Epoch 19 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.336740 loss[eng, spa]: 0.389108\n",
      "val_loss[eng, fra]: 0.286481\n",
      "val_loss[eng, spa]: 0.338884\n",
      "Time elapsed: 345.0735321044922 seconds\n",
      "Epoch 20 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.330497 loss[eng, spa]: 0.383888\n",
      "val_loss[eng, fra]: 0.282452\n",
      "val_loss[eng, spa]: 0.333465\n",
      "Time elapsed: 344.77323865890503 seconds\n",
      "Epoch 21 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.325310 loss[eng, spa]: 0.378974\n",
      "val_loss[eng, fra]: 0.278013\n",
      "val_loss[eng, spa]: 0.332872\n",
      "Time elapsed: 344.327668428421 seconds\n",
      "Epoch 22 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.319778 loss[eng, spa]: 0.373963\n",
      "val_loss[eng, fra]: 0.273574\n",
      "val_loss[eng, spa]: 0.325908\n",
      "Time elapsed: 29816.320325374603 seconds\n",
      "Epoch 23 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.315183 loss[eng, spa]: 0.369492\n",
      "val_loss[eng, fra]: 0.270502\n",
      "val_loss[eng, spa]: 0.323996\n",
      "Time elapsed: 336.7151069641113 seconds\n",
      "Epoch 24 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.310773 loss[eng, spa]: 0.364894\n",
      "val_loss[eng, fra]: 0.266114\n",
      "val_loss[eng, spa]: 0.315203\n",
      "Time elapsed: 333.62222933769226 seconds\n",
      "Epoch 25 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.306701 loss[eng, spa]: 0.359152\n",
      "val_loss[eng, fra]: 0.263295\n",
      "val_loss[eng, spa]: 0.310299\n",
      "Time elapsed: 331.74058961868286 seconds\n",
      "Epoch 26 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.303225 loss[eng, spa]: 0.353555\n",
      "val_loss[eng, fra]: 0.260551\n",
      "val_loss[eng, spa]: 0.305285\n",
      "Time elapsed: 348.17921686172485 seconds\n",
      "Epoch 27 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.299664 loss[eng, spa]: 0.348478\n",
      "val_loss[eng, fra]: 0.258341\n",
      "val_loss[eng, spa]: 0.300377\n",
      "Time elapsed: 335.15779089927673 seconds\n",
      "Epoch 28 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.296202 loss[eng, spa]: 0.343225\n",
      "val_loss[eng, fra]: 0.255280\n",
      "val_loss[eng, spa]: 0.295494\n",
      "Time elapsed: 334.2481117248535 seconds\n",
      "Epoch 29 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.292463 loss[eng, spa]: 0.339247\n",
      "val_loss[eng, fra]: 0.252274\n",
      "val_loss[eng, spa]: 0.291365\n",
      "Time elapsed: 333.99982166290283 seconds\n",
      "Epoch 30 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.289896 loss[eng, spa]: 0.334604\n",
      "val_loss[eng, fra]: 0.250438\n",
      "val_loss[eng, spa]: 0.287033\n",
      "Time elapsed: 334.61642932891846 seconds\n",
      "Epoch 31 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.287343 loss[eng, spa]: 0.330830\n",
      "val_loss[eng, fra]: 0.247836\n",
      "val_loss[eng, spa]: 0.284038\n",
      "Time elapsed: 334.6588206291199 seconds\n",
      "Epoch 32 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.285057 loss[eng, spa]: 0.326179\n",
      "val_loss[eng, fra]: 0.246004\n",
      "val_loss[eng, spa]: 0.279556\n",
      "Time elapsed: 334.4080867767334 seconds\n",
      "Epoch 33 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.281695 loss[eng, spa]: 0.322338\n",
      "val_loss[eng, fra]: 0.243083\n",
      "val_loss[eng, spa]: 0.275905\n",
      "Time elapsed: 334.7304639816284 seconds\n",
      "Epoch 34 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.279564 loss[eng, spa]: 0.319169\n",
      "val_loss[eng, fra]: 0.243184\n",
      "val_loss[eng, spa]: 0.273940\n",
      "Time elapsed: 334.5618839263916 seconds\n",
      "Epoch 35 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.277277 loss[eng, spa]: 0.316241\n",
      "val_loss[eng, fra]: 0.239816\n",
      "val_loss[eng, spa]: 0.269287\n",
      "Time elapsed: 334.58110427856445 seconds\n",
      "Epoch 36 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.274940 loss[eng, spa]: 0.312426\n",
      "val_loss[eng, fra]: 0.239844\n",
      "val_loss[eng, spa]: 0.267289\n",
      "Time elapsed: 334.4950120449066 seconds\n",
      "Epoch 37 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.273228 loss[eng, spa]: 0.310091\n",
      "val_loss[eng, fra]: 0.238363\n",
      "val_loss[eng, spa]: 0.267386\n",
      "Time elapsed: 334.434761762619 seconds\n",
      "Epoch 38 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.271314 loss[eng, spa]: 0.307769\n",
      "val_loss[eng, fra]: 0.235802\n",
      "val_loss[eng, spa]: 0.262821\n",
      "Time elapsed: 334.36025857925415 seconds\n",
      "Epoch 39 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.269096 loss[eng, spa]: 0.305102\n",
      "val_loss[eng, fra]: 0.235390\n",
      "val_loss[eng, spa]: 0.261777\n",
      "Time elapsed: 334.13325929641724 seconds\n",
      "Epoch 40 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.267992 loss[eng, spa]: 0.303041\n",
      "val_loss[eng, fra]: 0.233366\n",
      "val_loss[eng, spa]: 0.259916\n",
      "Time elapsed: 334.2454128265381 seconds\n",
      "Epoch 41 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.265852 loss[eng, spa]: 0.299551\n",
      "val_loss[eng, fra]: 0.232281\n",
      "val_loss[eng, spa]: 0.258636\n",
      "Time elapsed: 333.81035566329956 seconds\n",
      "Epoch 42 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.264144 loss[eng, spa]: 0.297852\n",
      "val_loss[eng, fra]: 0.229867\n",
      "val_loss[eng, spa]: 0.256227\n",
      "Time elapsed: 333.9568967819214 seconds\n",
      "Epoch 43 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.263303 loss[eng, spa]: 0.295339\n",
      "val_loss[eng, fra]: 0.229322\n",
      "val_loss[eng, spa]: 0.253669\n",
      "Time elapsed: 333.621258020401 seconds\n",
      "Epoch 44 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.260865 loss[eng, spa]: 0.293140\n",
      "val_loss[eng, fra]: 0.230420\n",
      "val_loss[eng, spa]: 0.252045\n",
      "Time elapsed: 333.9903709888458 seconds\n",
      "Epoch 45 of 48...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1750 of 1750: loss[eng, fra]: 0.259718 loss[eng, spa]: 0.291701\n",
      "val_loss[eng, fra]: 0.227235\n",
      "val_loss[eng, spa]: 0.249005\n",
      "Time elapsed: 332.70329427719116 seconds\n",
      "Epoch 46 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.258774 loss[eng, spa]: 0.289434\n",
      "val_loss[eng, fra]: 0.225957\n",
      "val_loss[eng, spa]: 0.247610\n",
      "Time elapsed: 332.39294028282166 seconds\n",
      "Epoch 47 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.257140 loss[eng, spa]: 0.287414\n",
      "val_loss[eng, fra]: 0.225161\n",
      "val_loss[eng, spa]: 0.247508\n",
      "Time elapsed: 332.52661085128784 seconds\n",
      "Epoch 48 of 48...\n",
      "Iteration 1750 of 1750: loss[eng, fra]: 0.255328 loss[eng, spa]: 0.285883\n",
      "val_loss[eng, fra]: 0.224718\n",
      "val_loss[eng, spa]: 0.246880\n",
      "Time elapsed: 332.378333568573 seconds\n"
     ]
    }
   ],
   "source": [
    "import sys, time\n",
    "\n",
    "epochs = 48\n",
    "\n",
    "history = {\n",
    "    p: {\n",
    "        'loss': [],\n",
    "        'val_loss': []\n",
    "    }\n",
    "    for p in clean_pairs\n",
    "}\n",
    "    \n",
    "for ep in range(epochs):\n",
    "    print(f'Epoch {ep+1} of {epochs}...')\n",
    "    \n",
    "    loss = {}\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    it = 0\n",
    "    for p, X, Y in gen_train:\n",
    "        it += 1\n",
    "        l = model[p].train_on_batch(X, Y)\n",
    "        \n",
    "        # Save the loss for metric computation\n",
    "        if p in loss:\n",
    "            loss[p].append(l)\n",
    "        else:\n",
    "            loss[p] = [l]\n",
    "        \n",
    "        # Print iteration info\n",
    "        if it % 10 == 0 or it == len(gen_train):\n",
    "            # Print iteration number\n",
    "            sys.stdout.write(f'\\rIteration {it} of {len(gen_train)}:')\n",
    "            \n",
    "            # Print losses\n",
    "            for q in clean_pairs:\n",
    "                if q in loss:\n",
    "                    l = sum(loss[q]) / len(loss[q])\n",
    "                    sys.stdout.write(f' loss[{q[0]}, {q[1]}]: {l:0.6f}')\n",
    "                \n",
    "            # Flush stdout buffer (display the text)\n",
    "            sys.stdout.flush()\n",
    "    \n",
    "    # Save training metrics\n",
    "    for p in loss:\n",
    "        history[p]['loss'].append(sum(loss[p]) / len(loss[p]))\n",
    "    \n",
    "    # Go to next line to print validation stats\n",
    "    sys.stdout.write('\\n')\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    # Compute the validation loss\n",
    "    for p in clean_pairs:\n",
    "        loss = model[p].evaluate(Xs_valid[p], Ys_valid[p], verbose=False)\n",
    "        print(f'val_loss[{p[0]}, {p[1]}]: {loss:0.6f}')\n",
    "        \n",
    "        # Save validation metrics\n",
    "        history[p]['val_loss'].append(loss)\n",
    "    \n",
    "    print('Time elapsed:', time.time() - start_time, 'seconds')\n",
    "\n",
    "    # Save the results\n",
    "    for p in clean_pairs:\n",
    "        model[p].save(f'seq2seq_{p[0]}_{p[1]}.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gHV3BVERo8F3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "for p in history:\n",
    "    loss = history[p]['loss']\n",
    "    val_loss = history[p]['val_loss']\n",
    "    \n",
    "    plt.plot(range(len(loss)), loss, 'bo', label='Training loss')\n",
    "    plt.plot(range(len(val_loss)), val_loss, 'ro', label='Validation loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Translation loss: {p[0]} to {p[1]}')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pj-W8t_DBhC-"
   },
   "source": [
    "## 4. Make predictions\n",
    "\n",
    "\n",
    "### 4.1. Translate English to many\n",
    "\n",
    "1. Encoder read a sentence (source language) and output its final states, $h_t$ and $c_t$.\n",
    "2. Take the [star] sign \"\\t\" and the final state $h_t$ and $c_t$ as input and run the decoder.\n",
    "3. Get the new states and predicted probability distribution.\n",
    "4. sample a char from the predicted probability distribution\n",
    "5. take the sampled char and the new states as input and repeat the process (stop if reach the [stop] sign \"\\n\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y3RxZmDXBhC-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reverse_input_char_index[eng]: {1: ' ', 2: 'e', 3: 't', 4: 'o', 5: 'i', 6: 'a', 7: 's', 8: 'h', 9: 'n', 10: 'r', 11: 'l', 12: 'd', 13: 'm', 14: 'y', 15: 'u', 16: 'w', 17: 'g', 18: 'c', 19: 'p', 20: 'k', 21: 'f', 22: 'b', 23: 'v', 24: 'j', 25: 'x', 26: 'z', 27: 'q', 28: 'é'}\n",
      "reverse_target_char_index[fra]: {1: ' ', 2: 'e', 3: 's', 4: 'a', 5: 't', 6: 'i', 7: 'u', 8: 'n', 9: 'o', 10: 'r', 11: '\\t', 12: '\\n', 13: 'l', 14: 'm', 15: 'p', 16: 'c', 17: 'd', 18: 'v', 19: 'é', 20: 'j', 21: 'q', 22: 'f', 23: 'b', 24: 'h', 25: 'g', 26: 'z', 27: 'x', 28: 'à', 29: 'ê', 30: 'è', 31: 'y', 32: 'ç', 33: 'ù', 34: 'ô', 35: 'î', 36: 'û', 37: 'â', 38: 'œ', 39: 'k', 40: 'ï', 41: 'w', 42: 'ë'}\n",
      "reverse_target_char_index[spa]: {1: ' ', 2: 'e', 3: 'a', 4: 'o', 5: 's', 6: 'n', 7: 'r', 8: 't', 9: '\\t', 10: '\\n', 11: 'l', 12: 'i', 13: 'u', 14: 'm', 15: 'd', 16: 'c', 17: 'p', 18: 'b', 19: 'v', 20: 'h', 21: 'g', 22: 'q', 23: 'é', 24: 'y', 25: 'á', 26: 'í', 27: 'ó', 28: 'f', 29: 'j', 30: 'z', 31: 'ñ', 32: 'ú', 33: 'x', 34: 'k', 35: 'w', 36: 'ü'}\n"
     ]
    }
   ],
   "source": [
    "# Reverse-lookup token index to decode sequences back to something readable.\n",
    "reverse_input_char_index = {\n",
    "    l: dict((i, char) for char, i in input_token_index[l].items())\n",
    "    for l in input_token_index\n",
    "}\n",
    "reverse_target_char_index = {\n",
    "    l: dict((i, char) for char, i in target_token_index[l].items())\n",
    "    for l in target_token_index\n",
    "}\n",
    "\n",
    "for l in reverse_input_char_index:\n",
    "    print(f'reverse_input_char_index[{l}]:', reverse_input_char_index[l])\n",
    "    \n",
    "for l in reverse_target_char_index:\n",
    "    print(f'reverse_target_char_index[{l}]:', reverse_target_char_index[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vgvg21quBhDE"
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq, trans, temperature=0.2):\n",
    "    src, tgt = trans\n",
    "    \n",
    "    states_value = encoder_model[src].predict(input_seq)\n",
    "    \n",
    "    num_dec_toks = num_decoder_tokens[tgt]\n",
    "\n",
    "    target_seq = numpy.zeros((1, 1))\n",
    "    target_seq[0, 0] = target_token_index[tgt]['\\t']\n",
    "    \n",
    "    reverse_target_index = reverse_target_char_index[tgt]\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model[tgt].predict([target_seq] + states_value)\n",
    "\n",
    "        # Get the probability of each character plus an epsilon\n",
    "        p = output_tokens[0, -1, :]\n",
    "        \n",
    "        # Apply temperature\n",
    "        p = numpy.log(p + 1e-16) / temperature\n",
    "        \n",
    "        # Rescale\n",
    "        p = numpy.exp(p.astype('float64'))\n",
    "        p = p / numpy.sum(p)\n",
    "        \n",
    "        # Randomly choose one from the distribution\n",
    "        p = numpy.random.multinomial(1, p, 1)\n",
    "        \n",
    "        # Choose the most likely character\n",
    "        sampled_token_index = numpy.argmax(p)\n",
    "        \n",
    "        if sampled_token_index in reverse_target_index:\n",
    "            sampled_char = reverse_target_index[sampled_token_index]\n",
    "            decoded_sentence += sampled_char\n",
    "        else:\n",
    "            sampled_char = ''\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length[tgt]):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = numpy.zeros((1, 1))\n",
    "        target_seq[0, 0] =  sampled_token_index\n",
    "\n",
    "        states_value[1:3] = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qvM6N9qGBhDV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "eng:         murder is a wicked crime\n",
      "fra (true):  le meurtre est un crime odieux\n",
      "fra (pred):  la mure est un crime de criment\n",
      "-\n",
      "eng:         i heard about your problems\n",
      "fra (true):  jai entendu parler de tes problèmes\n",
      "fra (pred):  jentends des problèmes de tes problèmes\n",
      "-\n",
      "eng:         i heard about your problems\n",
      "fra (true):  jai entendu parler de vos problèmes\n",
      "fra (pred):  jai entendu des problèmes de votre problème\n",
      "-\n",
      "eng:         you should do it\n",
      "fra (true):  tu devrais le faire\n",
      "fra (pred):  tu devrais le faire\n",
      "-\n",
      "eng:         you should do it\n",
      "fra (true):  vous devriez le faire\n",
      "fra (pred):  tu devrais le faire\n",
      "-\n",
      "eng:         why would anybody kiss me\n",
      "fra (true):  pourquoi quelquun membrasseraitil\n",
      "fra (pred):  pourquoi qui que ce soit de temps cela marchez\n",
      "-\n",
      "eng:         its not so easy\n",
      "fra (true):  ce nest pas si facile\n",
      "fra (pred):  ce nest pas si facile\n",
      "-\n",
      "eng:         youre an idiot\n",
      "fra (true):  tu es un idiot\n",
      "fra (pred):  tu es un idiot\n",
      "-\n",
      "eng:         tom wants to come with us\n",
      "fra (true):  tom veut aller avec nous\n",
      "fra (pred):  tom veut venir avec nous\n",
      "-\n",
      "eng:         i bookmarked this website\n",
      "fra (true):  jai créé des signets vers ce site\n",
      "fra (pred):  jai absorté cette voiture\n",
      "-\n",
      "eng:         he held her tightly\n",
      "fra (true):  il la fermement tenue\n",
      "fra (pred):  il la dit à son tiche\n",
      "-\n",
      "eng:         he held her tightly\n",
      "fra (true):  il la tint fermement\n",
      "fra (pred):  il la dit à la taille\n",
      "-\n",
      "eng:         ill go change my clothes\n",
      "fra (true):  je vais me changer\n",
      "fra (pred):  je changerai mon changement\n",
      "-\n",
      "eng:         i thought he was honest\n",
      "fra (true):  je pensais quil était honnête\n",
      "fra (pred):  je pensais quil était honnête\n",
      "-\n",
      "eng:         i thought he was honest\n",
      "fra (true):  jai pensé quil était honnête\n",
      "fra (pred):  je pensais quil était honnête\n",
      "-\n",
      "eng:         please tell tom im sorry\n",
      "fra (true):  dites à tom que je suis désolée sil vous plaît\n",
      "fra (pred):  disje tout ce jour plus darie\n",
      "-\n",
      "eng:         please tell tom im sorry\n",
      "fra (true):  dis à tom que je suis désolé sil te plaît\n",
      "fra (pred):  dises à tom je suis désolée de sil vous plaît\n",
      "-\n",
      "eng:         were in trouble\n",
      "fra (true):  on est dans le pétrin\n",
      "fra (pred):  nous sommes en train de probobler\n",
      "-\n",
      "eng:         were in trouble\n",
      "fra (true):  nous sommes dans le pétrin\n",
      "fra (pred):  nous sommes au trouble\n",
      "-\n",
      "eng:         i had some things to do\n",
      "fra (true):  jai eu des choses à faire\n",
      "fra (pred):  jai eu quelque chose à faire\n",
      "-\n",
      "eng:         what are they doing\n",
      "spa (true):  hacen\n",
      "spa (pred):  hacen eso\n",
      "-\n",
      "eng:         my fathers hobby is fishing\n",
      "spa (true):  el pasatiempo de mi padre es la pesca\n",
      "spa (pred):  mi padre está pesado pasado\n",
      "-\n",
      "eng:         id like to wring toms neck\n",
      "spa (true):  me gustaría retorcerle el cuello a tom\n",
      "spa (pred):  quisiera verle a tom en el necesio\n",
      "-\n",
      "eng:         he changed a few words\n",
      "spa (true):  ha cambiado algunas palabras\n",
      "spa (pred):  él compró una mante suerte\n",
      "-\n",
      "eng:         keep searching\n",
      "spa (true):  sigue buscando\n",
      "spa (pred):  sigue sentado\n",
      "-\n",
      "eng:         how awful\n",
      "spa (true):  horror\n",
      "spa (pred):  contiguad\n",
      "-\n",
      "eng:         it belonged to tom\n",
      "spa (true):  pertenecía a tomás\n",
      "spa (pred):  se contó a tom\n",
      "-\n",
      "eng:         im learning chinese\n",
      "spa (true):  estoy aprendiendo chino\n",
      "spa (pred):  estoy aprendiendo chino\n",
      "-\n",
      "eng:         i made breakfast\n",
      "spa (true):  hice el desayuno\n",
      "spa (pred):  hice en crecionado\n",
      "-\n",
      "eng:         are you nuts\n",
      "spa (true):  loco\n",
      "spa (pred):  estás enfermo\n",
      "-\n",
      "eng:         tom walked away unhurt\n",
      "spa (true):  tom se marchó ileso\n",
      "spa (pred):  tom se caminó a ayudar\n",
      "-\n",
      "eng:         i didnt see their faces\n",
      "spa (true):  no vi sus caras\n",
      "spa (pred):  no vi a tenir sus dados\n",
      "-\n",
      "eng:         give this copy to tom\n",
      "spa (true):  dale esta copia a tom\n",
      "spa (pred):  dale a tom a tom\n",
      "-\n",
      "eng:         give this copy to tom\n",
      "spa (true):  entrégale esta copia a tom\n",
      "spa (pred):  dale esto a tom\n",
      "-\n",
      "eng:         tom might get lost\n",
      "spa (true):  tom quizás podría perderse\n",
      "spa (pred):  tom podría ver a perder\n",
      "-\n",
      "eng:         tom makes big money\n",
      "spa (true):  tom gana mucho dinero\n",
      "spa (pred):  tom compra mintimado\n",
      "-\n",
      "eng:         it will burn\n",
      "spa (true):  se quemará\n",
      "spa (pred):  se vendrá\n",
      "-\n",
      "eng:         dancing is not a crime\n",
      "spa (true):  bailar no es un crimen\n",
      "spa (pred):  escuchar no es una crima\n",
      "-\n",
      "eng:         i hate winter\n",
      "spa (true):  odio el invierno\n",
      "spa (pred):  odio el contrator\n",
      "-\n",
      "eng:         we have no clue where he is\n",
      "spa (true):  no tenemos idea de dónde está él\n",
      "spa (pred):  no tenemos coco aquí de nodo\n"
     ]
    }
   ],
   "source": [
    "for l in pairs:\n",
    "    for seq_index in range(2100, 2120):\n",
    "        # Take one sequence (part of the training set)\n",
    "        # for trying out decoding.\n",
    "        input_seq = encoder_input_seq[l][seq_index: seq_index + 1]\n",
    "        decoded_sentence = decode_sequence(input_seq, l)\n",
    "        print('-')\n",
    "        print(f'{l[0]}:        ', input_texts[l][seq_index])\n",
    "        print(f'{l[1]} (true): ', target_texts[l][seq_index][1:-1])\n",
    "        print(f'{l[1]} (pred): ', decoded_sentence[0:-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sp_bFS9GBhDl"
   },
   "source": [
    "### 4.2. Translate an English sentence to the target language\n",
    "\n",
    "1. Tokenization\n",
    "2. One-hot encode\n",
    "3. Translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7MCnDi9QBhDm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source sentence is: why is that\n",
      "translated sentence in fra is: pourquoi ça\n",
      "translated sentence in spa is: qué es eso\n"
     ]
    }
   ],
   "source": [
    "input_sentence = 'why is that'\n",
    "print('source sentence is: ' + input_sentence)\n",
    "\n",
    "def translate(t, a, b, temperature=0.2):\n",
    "    # Tokenize the text\n",
    "    input_sequence = encode_text([t], encoder_tokenizer[a], max_encoder_seq_length[a])\n",
    "    \n",
    "    # Evaluate the translation\n",
    "    translated_sentence = decode_sequence(input_sequence, (a,b), temperature=0.2)\n",
    "    \n",
    "    return translated_sentence\n",
    "\n",
    "for l in pairs:\n",
    "    if l[0] != 'eng':\n",
    "        continue\n",
    "    \n",
    "    # Evaluate and get the sentence\n",
    "    translated_sentence = translate(input_sentence, l[0], l[1])\n",
    "    print(f'translated sentence in {l[1]} is: ' + translated_sentence.strip())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uj5ABe5aBhEH"
   },
   "source": [
    "## 5. Evaluate the translation using BLEU score\n",
    "\n",
    "Reference: \n",
    "- https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
    "- https://en.wikipedia.org/wiki/BLEU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V7p5fjBOBhEH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building eng to fra reference\n",
      "Generating candidates\n",
      "Computing BLEU score\n",
      "bleu_score[eng, fra]: 0.0429645604749982\n",
      "Building eng to spa reference\n",
      "Generating candidates\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "for p in pairs:\n",
    "    dpairs = dirty_pairs[p]\n",
    "    \n",
    "    # Compute the set of expected vs actual results\n",
    "    expected = {}\n",
    "    print(f'Building {p[0]} to {p[1]} reference')\n",
    "    for src, tgt in dpairs:\n",
    "        if src not in expected:\n",
    "            expected[src] = [tgt.split()]\n",
    "        else:\n",
    "            expected[src] += [tgt.split()]\n",
    "            \n",
    "    # Get the expected strings\n",
    "    references = [expected[s] for s in expected]\n",
    "    print('Found', len(references), 'reference translations')\n",
    "    \n",
    "    # Translate the source strings\n",
    "    print('Generating candidates')\n",
    "    candidates = [translate(s, p[0], p[1]).split()\n",
    "                  for s in tqdm(expected, desc=f'Translating from {p[0]} to {p[1]}')]\n",
    "    \n",
    "    # Compute the BLEU score\n",
    "    print('Computing BLEU score')\n",
    "    score = corpus_bleu(references, candidates)\n",
    "    \n",
    "    print(f'bleu_score[{p[0]}, {p[1]}]: {score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "seq2seq.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
